{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_network-w/numpy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPrvLZ3LnU8r",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression using Numpy\n",
        "**Implementation** of a logistic regression model that can predict cat images only using numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDlLA4awmwMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXgVg7RzoIxG",
        "colab_type": "text"
      },
      "source": [
        "We load a dataset using pandas or directly from Google Drive on-to google Colab by the following code: Just a placeholder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBtz9XsvoRz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# train_set_x_orig = Original Training Set     --------- we gonna copy and then-reshape while doing NN Operations\n",
        "# test_set_x_orig = Original Test Set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cALRHZMDoeJQ",
        "colab_type": "text"
      },
      "source": [
        "Lets say we have values like the following pattern:\n",
        "\n",
        "    - m_train (number of training examples)\n",
        "    - m_test (number of test examples)\n",
        "    - num_px (= height = width of a training image)\n",
        "\n",
        "And we have a `train_set_x_orig` named numpy-array of shape (m_train, num_px, num_px, 3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coK1zWsWoe0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m_train = len(train_set_x_orig)\n",
        "m_test = len(test_set_x_orig)\n",
        "num_px = np.shape(train_set_x_orig[1])[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJlq3oSQplU6",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid function\n",
        "It's often used in as activation function\n",
        "\n",
        "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ \n",
        "\n",
        "is sometimes also known as the logistic function. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning.\n",
        "\n",
        "I here implemented exponentials as `np.exp(...)` because if your input features are Nx X M dimentional Vectors which they obviously are most of the time, `np.exp(...)` uses SIMD (single instruction multiple data) techniques to parallelly run instructions on GPU and CPUs. This makes the code significantly faster.\n",
        "\n",
        "In the later portions of this notebook I've shown the time difference of a explicit `for loop` and a SIMD instruction implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOYTVKofply7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):        \n",
        "    sigmoid_out = 1/(1+np.exp(-x))\n",
        "    return sigmoid_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cDM3lrcqmyS",
        "colab_type": "code",
        "outputId": "f944bed3-62f0-46a0-e008-e420e05f9323",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,1,2,3]))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigmoid([0, 2]) = [0.5        0.73105858 0.88079708 0.95257413]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTQbsMiaqgMw",
        "colab_type": "text"
      },
      "source": [
        "## Derivative of the Sigmoid Out\n",
        "One of the main reasons why we use sigmoid functions as a popular activation function is because it's monotonous and bounded and has a simple derivative.\n",
        "### Sigmoid gradient calculation\n",
        "\n",
        "You will need to compute gradients to optimize loss functions using backpropagation. Let's code your first gradient function.\n",
        "\n",
        "We here implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. The formula is: $$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{Derivative}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cycdhFZsrUVY",
        "colab_type": "text"
      },
      "source": [
        "## Next is initialization of w,b as zeros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHUX1woarSeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    dim -- size of the w vector we want (or number of parameters in this case)\n",
        "    \n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (corresponds to the bias)\n",
        "    \"\"\"\n",
        "    w = np.zeros((dim,1))\n",
        "    b = 0\n",
        "    \n",
        "    assert(w.shape == (dim, 1))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q5fshxGV7ve",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "24f43a2d-d652-4ea5-8bc3-b669bf4b9c4a"
      },
      "source": [
        "dim = 2\n",
        "w, b = initialize_with_zeros(dim)\n",
        "print (\"w = \" + str(w))\n",
        "print (\"b = \" + str(b))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = [[0.]\n",
            " [0.]]\n",
            "b = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP4_xNwDWKPf",
        "colab_type": "text"
      },
      "source": [
        "## Forward and Backward propagation\n",
        "\n",
        "Now that our parameters are initialized, we implement the \"forward\" and \"backward\" propagation steps for learning the parameters.\n",
        "\n",
        "Forward Propagation:\n",
        "- We get X\n",
        "- We then compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
        "- Next calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
        "\n",
        "Here are the two formulas we will be using below: \n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
        "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0GGAdxqWB8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)  Number of example m = x.shape[1]\n",
        "    Y -- true \"label\" vector\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    \n",
        "    A = sigmoid(w.T @ X + b)     # compute activation               \n",
        "    # Computation of cost\n",
        "    cost = -1 / m * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A)) \n",
        "    \n",
        "    # BACKWARD PROPAGATION BELOW\n",
        "    dw = (1/m) * (X @ (A-Y).T) \n",
        "    db = (1/m) * (np.sum(A - Y))\n",
        "\n",
        "    assert(dw.shape == w.shape)\n",
        "    assert(db.dtype == float)\n",
        "    cost = np.squeeze(cost)\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return grads, cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kHLb7s4XCjt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "517dbce0-92da-48e6-b7db-94c5b61e73c5"
      },
      "source": [
        "# Example:\n",
        "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
        "grads, cost = propagate(w, b, X, Y)\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"cost = \" + str(cost))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dw = [[0.99845601]\n",
            " [2.39507239]]\n",
            "db = 0.001455578136784208\n",
            "cost = 5.801545319394553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRL-OhAvXSX6",
        "colab_type": "text"
      },
      "source": [
        "## Optimization\n",
        "- We have initialized your parameters.\n",
        "- We can compute a cost function and its gradient.\n",
        "- Now, we going to update the parameters using gradient descent.\n",
        "\n",
        "The **goal** is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaQ1bwnIXE0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        a = propagate(w,b,X,Y)\n",
        "        grads, cost = a[0],a[1]\n",
        "        \n",
        "        # Retrieval of derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        w = w - learning_rate* dw\n",
        "        b = b - learning_rate*db\n",
        "        \n",
        "        # Cost Record\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "    \n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk_ivxQ6X-GQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a6554044-c648-4334-fa07-eae80048cf01"
      },
      "source": [
        "# Example:\n",
        "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = [[0.19033591]\n",
            " [0.12259159]]\n",
            "b = 1.9253598300845747\n",
            "dw = [[0.67752042]\n",
            " [1.41625495]]\n",
            "db = 0.21919450454067657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvx-idBkYJfC",
        "colab_type": "text"
      },
      "source": [
        "We will now implement the `predict()` function. There are two steps to computing predictions:\n",
        "\n",
        "1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
        "\n",
        "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`. We can used an `if`/`else` statement in a `for` loop (though there is also a way to vectorize this). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1ceL4CcYCrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "\n",
        "    A = sigmoid(w.T @ X + b)\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "        \n",
        "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
        "        if A[0,i] > 0.5:\n",
        "            Y_prediction[0,i] = 1\n",
        "        else:\n",
        "            Y_prediction[0,i] = 0\n",
        "                \n",
        "    assert(Y_prediction.shape == (1, m))\n",
        "    \n",
        "    return Y_prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EjdO3X1YlFT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f39c9c3a-c55f-4b16-b673-c8a4d2cb5508"
      },
      "source": [
        "# Prediction Example:\n",
        "w = np.array([[0.1124579],[0.23106775]])\n",
        "b = -0.3\n",
        "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
        "print (\"predictions = \" + str(predict(w, b, X)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions = [[1. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsGqtmOmYvRh",
        "colab_type": "text"
      },
      "source": [
        "## Merge all functions into a model\n",
        "\n",
        "You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n",
        "\n",
        "We used the following notation:\n",
        "\n",
        "1.   Y_prediction_test for your predictions on the test set\n",
        "2.   Y_prediction_train for your predictions on the train set\n",
        "3.   w, costs, grads for the outputs of optimize()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZdq79P_Yr7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # initialization of parameters with zeros\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "\n",
        "    # Gradient descent\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    \n",
        "    # Retrieval of parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # Predict test/train set examples\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "    \n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    \n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}